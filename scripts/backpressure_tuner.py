"""
èƒŒåœ§ã‚ªãƒ¼ãƒˆãƒãƒ¥ãƒ¼ãƒŠ (Backpressure Auto-Tuner)

ç›®çš„:
  /metricsã‹ã‚‰ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’èª­ã¿å–ã‚Šã€consumer lagã€fetch latencyã€retry rateã€
  drop countãªã©ã‚’åˆ†æã—ã¦ã€Kafkaãƒ—ãƒ­ãƒ‡ãƒ¥ãƒ¼ã‚µãƒ¼/ã‚³ãƒ³ã‚·ãƒ¥ãƒ¼ãƒãƒ¼ã®æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ¨å¥¨ã™ã‚‹ã€‚

å‡ºåŠ›:
  - CSVå½¢å¼ã®æ¨å¥¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ (out/backpressure_tuner.csv)
  - PRã‚³ãƒ¡ãƒ³ãƒˆç”¨ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ (ã‚ªãƒ—ã‚·ãƒ§ãƒ³: --pr-comment)

ä½¿ç”¨ä¾‹:
  # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã‹ã‚‰æ¨å¥¨ã‚’ç”Ÿæˆ
  python scripts/backpressure_tuner.py --metrics var/results/canary.json --out var/results/tuner.csv

  # PRã‚³ãƒ¡ãƒ³ãƒˆç”Ÿæˆ
  python scripts/backpressure_tuner.py --metrics var/results/canary.json --pr-comment var/results/tuner_comment.md
"""

import json
import sys
import argparse
import csv
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass


@dataclass
class MetricsInput:
    """å…¥åŠ›ãƒ¡ãƒˆãƒªã‚¯ã‚¹"""
    fast_path_count: int
    drop_count: int
    process_p99_us: float
    persistence_queue_lag: int
    persistence_queue_error_count: int
    persistence_queue_write_p99_us: float
    throughput_events_per_sec: float


@dataclass
class TunerRecommendation:
    """ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¨å¥¨"""
    parameter: str
    current_value: Optional[str]
    recommended_value: str
    reason: str
    priority: str  # high, medium, low


class BackpressureTuner:
    """èƒŒåœ§ã‚ªãƒ¼ãƒˆãƒãƒ¥ãƒ¼ãƒŠ"""

    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ (Kafka/Fast Path)
    DEFAULT_LINGER_MS = 10
    DEFAULT_BATCH_SIZE = 16384
    DEFAULT_MAX_IN_FLIGHT = 5
    DEFAULT_BUFFER_SIZE = 65536
    DEFAULT_MAX_POLL_INTERVAL_MS = 300000

    # SLOé–¾å€¤ (docs/specs/slo.mdã‹ã‚‰)
    SLO_P99_US = 100.0
    SLO_DROP_COUNT = 0
    SLO_THROUGHPUT_MIN = 10000
    SLO_PQ_LAG_MAX = 1000
    SLO_ERROR_RATE_MAX = 0.01

    def __init__(self, metrics: MetricsInput):
        self.metrics = metrics
        self.recommendations: List[TunerRecommendation] = []

    def analyze_and_recommend(self) -> List[TunerRecommendation]:
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’åˆ†æã—ã€æ¨å¥¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ"""
        self._check_drop_count()
        self._check_persistence_queue_lag()
        self._check_latency()
        self._check_error_rate()
        self._check_throughput()

        return self.recommendations

    def _check_drop_count(self):
        """ãƒ‰ãƒ­ãƒƒãƒ—æ•°ãƒã‚§ãƒƒã‚¯: Fast Pathãƒãƒƒãƒ•ã‚¡æº¢ã‚Œ"""
        if self.metrics.drop_count > 0:
            # Fast Pathãƒªãƒ³ã‚°ãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚ºæ‹¡å¤§ã‚’æ¨å¥¨
            self.recommendations.append(TunerRecommendation(
                parameter="FAST_PATH_BUFFER_SIZE",
                current_value=str(self.DEFAULT_BUFFER_SIZE),
                recommended_value=str(self.DEFAULT_BUFFER_SIZE * 2),
                reason=f"ãƒ‰ãƒ­ãƒƒãƒ—æ¤œå‡º: {self.metrics.drop_count}ä»¶ã€‚ãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚º2å€åŒ–ã§ãƒãƒ¼ã‚¹ãƒˆå¸å",
                priority="high"
            ))

            # Persistence Queueå‡¦ç†èƒ½åŠ›å‘ä¸Š (Batch sizeæ‹¡å¤§)
            self.recommendations.append(TunerRecommendation(
                parameter="CHRONICLE_BATCH_SIZE",
                current_value="1",
                recommended_value="10",
                reason="Persistence Queueå‡¦ç†é…å»¶ã€‚ãƒãƒƒãƒæ›¸ãè¾¼ã¿ã§æ›¸ãè¾¼ã¿ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆå‘ä¸Š",
                priority="high"
            ))

    def _check_persistence_queue_lag(self):
        """Persistence Queueãƒ©ã‚°ãƒã‚§ãƒƒã‚¯"""
        if self.metrics.persistence_queue_lag > self.SLO_PQ_LAG_MAX:
            # Chronicle Queueæ›¸ãè¾¼ã¿ä¸¦åˆ—åº¦å‘ä¸Š
            self.recommendations.append(TunerRecommendation(
                parameter="PERSISTENCE_QUEUE_THREADS",
                current_value="1",
                recommended_value="2",
                reason=f"Persistence Queueãƒ©ã‚°é«˜é¨°: {self.metrics.persistence_queue_lag}ä»¶ > {self.SLO_PQ_LAG_MAX}ä»¶ã€‚ä¸¦åˆ—æ›¸ãè¾¼ã¿ã§æ”¹å–„",
                priority="high"
            ))

    def _check_latency(self):
        """ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒã‚§ãƒƒã‚¯: p99ãŒSLOè¶…é"""
        if self.metrics.process_p99_us > self.SLO_P99_US:
            overage_pct = ((self.metrics.process_p99_us - self.SLO_P99_US) / self.SLO_P99_US) * 100

            # GCèª¿æ•´æ¨å¥¨
            self.recommendations.append(TunerRecommendation(
                parameter="JVM_GC_PAUSE_TARGET",
                current_value="1ms",
                recommended_value="0.5ms",
                reason=f"p99è¶…é: {self.metrics.process_p99_us:.1f}Î¼s > {self.SLO_P99_US}Î¼s (+{overage_pct:.1f}%)ã€‚GC pause targetå¼•ãä¸‹ã’",
                priority="medium"
            ))

            # YieldingWaitStrategy -> BusySpinWaitStrategy (è¶…ä½ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·å„ªå…ˆ)
            self.recommendations.append(TunerRecommendation(
                parameter="FAST_PATH_WAIT_STRATEGY",
                current_value="YieldingWaitStrategy",
                recommended_value="BusySpinWaitStrategy",
                reason="ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·SLOæœªé”ã€‚BusySpinã§å¾…æ©Ÿã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰å‰Šæ¸› (CPUä½¿ç”¨ç‡ã¯ä¸Šæ˜‡)",
                priority="low"
            ))

    def _check_error_rate(self):
        """ã‚¨ãƒ©ãƒ¼ç‡ãƒã‚§ãƒƒã‚¯"""
        if self.metrics.fast_path_count > 0:
            error_rate = (self.metrics.persistence_queue_error_count / self.metrics.fast_path_count) * 100
        else:
            error_rate = 0.0

        if error_rate > self.SLO_ERROR_RATE_MAX:
            # Chronicle Queueæ›¸ãè¾¼ã¿ã‚¨ãƒ©ãƒ¼: ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡/æ¨©é™ãƒã‚§ãƒƒã‚¯æ¨å¥¨
            self.recommendations.append(TunerRecommendation(
                parameter="CHRONICLE_QUEUE_PATH",
                current_value="var/chronicle",
                recommended_value="æ¤œè¨¼ãŒå¿…è¦",
                reason=f"Chronicleæ›¸ãè¾¼ã¿ã‚¨ãƒ©ãƒ¼ç‡: {error_rate:.2f}% > {self.SLO_ERROR_RATE_MAX}%ã€‚ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡/æ¨©é™ã‚’ç¢ºèª",
                priority="high"
            ))

    def _check_throughput(self):
        """ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆãƒã‚§ãƒƒã‚¯"""
        if self.metrics.throughput_events_per_sec < self.SLO_THROUGHPUT_MIN:
            shortfall_pct = ((self.SLO_THROUGHPUT_MIN - self.metrics.throughput_events_per_sec) / self.SLO_THROUGHPUT_MIN) * 100

            # Kafkaãƒ—ãƒ­ãƒ‡ãƒ¥ãƒ¼ã‚µãƒ¼linger.mså‰Šæ¸› (ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·å„ªå…ˆ)
            self.recommendations.append(TunerRecommendation(
                parameter="KAFKA_LINGER_MS",
                current_value=str(self.DEFAULT_LINGER_MS),
                recommended_value="1",
                reason=f"ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆä¸è¶³: {self.metrics.throughput_events_per_sec:.0f} < {self.SLO_THROUGHPUT_MIN} events/s (-{shortfall_pct:.1f}%)ã€‚linger.mså‰Šæ¸›ã§é€ä¿¡é…å»¶çŸ­ç¸®",
                priority="medium"
            ))


def load_metrics_from_file(file_path: Path) -> MetricsInput:
    """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœJSONã‹ã‚‰ãƒ¡ãƒˆãƒªã‚¯ã‚¹æŠ½å‡º"""
    with open(file_path, 'r') as f:
        data = json.load(f)

    metrics = data.get("metrics", {})
    fast_path = metrics.get("fast_path", {})
    process_latency = fast_path.get("process_latency_us", {})
    persistence_queue = metrics.get("persistence_queue", {})
    summary = metrics.get("summary", {})

    return MetricsInput(
        fast_path_count=fast_path.get("count", 0),
        drop_count=fast_path.get("drop_count", 0),
        process_p99_us=process_latency.get("p99", 0.0),
        persistence_queue_lag=persistence_queue.get("lag", 0),
        persistence_queue_error_count=persistence_queue.get("error_count", 0),
        persistence_queue_write_p99_us=persistence_queue.get("write_latency_us", {}).get("p99", 0.0),
        throughput_events_per_sec=summary.get("throughput_events_per_sec", 0.0)
    )


def write_csv(recommendations: List[TunerRecommendation], output_path: Path):
    """CSVå½¢å¼ã§æ¨å¥¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å‡ºåŠ›"""
    output_path.parent.mkdir(parents=True, exist_ok=True)

    with open(output_path, 'w', newline='') as csvfile:
        fieldnames = ['priority', 'parameter', 'current_value', 'recommended_value', 'reason']
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)

        writer.writeheader()
        for rec in sorted(recommendations, key=lambda r: {'high': 0, 'medium': 1, 'low': 2}[r.priority]):
            writer.writerow({
                'priority': rec.priority,
                'parameter': rec.parameter,
                'current_value': rec.current_value or "N/A",
                'recommended_value': rec.recommended_value,
                'reason': rec.reason
            })


def generate_pr_comment(recommendations: List[TunerRecommendation]) -> str:
    """PRã‚³ãƒ¡ãƒ³ãƒˆç”¨ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ç”Ÿæˆ"""
    if not recommendations:
        return "âœ… **èƒŒåœ§ã‚ªãƒ¼ãƒˆãƒãƒ¥ãƒ¼ãƒŠ**: ã™ã¹ã¦ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒæ­£å¸¸ç¯„å›²å†…ã§ã™ã€‚æ¨å¥¨äº‹é …ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚"

    lines = [
        "## èƒŒåœ§ã‚ªãƒ¼ãƒˆãƒãƒ¥ãƒ¼ãƒŠæ¨å¥¨äº‹é …",
        "",
        "ä»¥ä¸‹ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ã‚’æ¨å¥¨ã—ã¾ã™:",
        "",
        "| å„ªå…ˆåº¦ | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | ç¾åœ¨å€¤ | æ¨å¥¨å€¤ | ç†ç”± |",
        "|--------|-----------|--------|--------|------|"
    ]

    priority_emoji = {
        'high': 'ğŸ”´',
        'medium': 'ğŸŸ¡',
        'low': 'âšª'
    }

    for rec in sorted(recommendations, key=lambda r: {'high': 0, 'medium': 1, 'low': 2}[r.priority]):
        emoji = priority_emoji.get(rec.priority, '')
        lines.append(f"| {emoji} {rec.priority.upper()} | `{rec.parameter}` | {rec.current_value or 'N/A'} | **{rec.recommended_value}** | {rec.reason} |")

    lines.append("")
    lines.append("### é©ç”¨æ–¹æ³•")
    lines.append("")
    lines.append("```bash")
    lines.append("# ç’°å¢ƒå¤‰æ•°ã¨ã—ã¦è¨­å®š")
    for rec in recommendations:
        if rec.parameter.startswith("FAST_PATH_") or rec.parameter.startswith("KAFKA_") or rec.parameter.startswith("CHRONICLE_"):
            lines.append(f"export {rec.parameter}={rec.recommended_value}")
    lines.append("```")
    lines.append("")
    lines.append("**æ³¨æ„**: æœ¬æ¨å¥¨ã¯è‡ªå‹•ç”Ÿæˆã§ã™ã€‚é©ç”¨å‰ã«å¿…ãšã‚¹ãƒ†ãƒ¼ã‚¸ãƒ³ã‚°ç’°å¢ƒã§æ¤œè¨¼ã—ã¦ãã ã•ã„ã€‚")

    return "\n".join(lines)


def main():
    parser = argparse.ArgumentParser(description="èƒŒåœ§ã‚ªãƒ¼ãƒˆãƒãƒ¥ãƒ¼ãƒŠ: Kafkaãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¨å¥¨ç”Ÿæˆ")
    parser.add_argument("--metrics", required=True, help="å…¥åŠ›ãƒ¡ãƒˆãƒªã‚¯ã‚¹JSON (ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ)")
    parser.add_argument("--out", help="å‡ºåŠ›CSV (ä¾‹: var/results/tuner.csv)")
    parser.add_argument("--pr-comment", help="PRã‚³ãƒ¡ãƒ³ãƒˆç”¨ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å‡ºåŠ› (ä¾‹: var/results/tuner_comment.md)")
    parser.add_argument("--verbose", action="store_true", help="è©³ç´°å‡ºåŠ›")

    args = parser.parse_args()

    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹èª­ã¿è¾¼ã¿
    metrics_path = Path(args.metrics)
    if not metrics_path.exists():
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {metrics_path}", file=sys.stderr)
        sys.exit(1)

    metrics = load_metrics_from_file(metrics_path)

    if args.verbose:
        print("ğŸ“Š å…¥åŠ›ãƒ¡ãƒˆãƒªã‚¯ã‚¹:")
        print(f"   Fast Pathå‡¦ç†æ•°: {metrics.fast_path_count}")
        print(f"   ãƒ‰ãƒ­ãƒƒãƒ—æ•°: {metrics.drop_count}")
        print(f"   p99ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·: {metrics.process_p99_us:.1f}Î¼s")
        print(f"   Persistence Queue Lag: {metrics.persistence_queue_lag}")
        print(f"   ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {metrics.throughput_events_per_sec:.0f} events/s")
        print()

    # ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¨å¥¨ç”Ÿæˆ
    tuner = BackpressureTuner(metrics)
    recommendations = tuner.analyze_and_recommend()

    if not recommendations:
        print("âœ… ã™ã¹ã¦ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒæ­£å¸¸ç¯„å›²å†…ã§ã™ã€‚æ¨å¥¨äº‹é …ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
        # ç©ºCSVã‚’å‡ºåŠ› (CIå‘ã‘)
        if args.out:
            write_csv([], Path(args.out))
        sys.exit(0)

    # CSVå‡ºåŠ›
    if args.out:
        output_path = Path(args.out)
        write_csv(recommendations, output_path)
        print(f"ğŸ’¾ æ¨å¥¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿CSVå‡ºåŠ›: {output_path}")

    # PRã‚³ãƒ¡ãƒ³ãƒˆç”Ÿæˆ
    if args.pr_comment:
        comment_path = Path(args.pr_comment)
        comment_path.parent.mkdir(parents=True, exist_ok=True)
        comment = generate_pr_comment(recommendations)
        with open(comment_path, 'w') as f:
            f.write(comment)
        print(f"ğŸ’¬ PRã‚³ãƒ¡ãƒ³ãƒˆç”Ÿæˆ: {comment_path}")

    # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«å‡ºåŠ›
    print()
    print("ğŸ”§ èƒŒåœ§ã‚ªãƒ¼ãƒˆãƒãƒ¥ãƒ¼ãƒŠæ¨å¥¨:")
    for rec in sorted(recommendations, key=lambda r: {'high': 0, 'medium': 1, 'low': 2}[r.priority]):
        priority_color = {
            'high': '\033[91m',     # èµ¤
            'medium': '\033[93m',   # é»„
            'low': '\033[90m'       # ã‚°ãƒ¬ãƒ¼
        }
        color = priority_color.get(rec.priority, '')
        reset = '\033[0m'
        print(f"  {color}[{rec.priority.upper()}]{reset} {rec.parameter}: {rec.current_value} â†’ {rec.recommended_value}")
        print(f"       ç†ç”±: {rec.reason}")

    print()
    print(f"âœ… åˆè¨ˆ {len(recommendations)} ä»¶ã®æ¨å¥¨äº‹é …ã‚’ç”Ÿæˆ")


if __name__ == "__main__":
    main()
