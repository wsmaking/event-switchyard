#!/usr/bin/env python3
"""
ã‚¯ã‚¤ãƒƒã‚¯ã‚«ãƒŠãƒªãƒ¼ãƒ†ã‚¹ãƒˆ: ç°¡æ˜“ç‰ˆSLOæ¤œè¨¼

ç›®çš„:
  èµ·å‹•ä¸­ã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã«å¯¾ã—ã¦è»½é‡ãªãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯ã‚’é€ä¿¡ã—ã€
  /statsã‹ã‚‰ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å–å¾—ã—ã¦ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœJSONã‚’ç”Ÿæˆã™ã‚‹ã€‚

ä½¿ç”¨ä¾‹:
  python scripts/quick_canary.py \
    --url http://localhost:8080 \
    --events 1000 \
    --out var/results/canary.json
"""

import json
import sys
import argparse
import time
import random
from pathlib import Path
from datetime import datetime, timezone
import urllib.request
import urllib.error


def send_event(url: str, key: str, payload: dict) -> bool:
    """ã‚¤ãƒ™ãƒ³ãƒˆé€ä¿¡"""
    try:
        data = json.dumps(payload).encode('utf-8')
        headers = {
            'Content-Type': 'application/json'
        }

        req = urllib.request.Request(f"{url}/events?key={key}", data=data, headers=headers, method='POST')
        with urllib.request.urlopen(req, timeout=5) as response:
            return response.status == 200
    except Exception:
        return False


def get_stats(url: str) -> dict:
    """çµ±è¨ˆæƒ…å ±å–å¾—"""
    try:
        req = urllib.request.Request(f"{url}/stats")
        with urllib.request.urlopen(req, timeout=5) as response:
            return json.loads(response.read().decode('utf-8'))
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: çµ±è¨ˆæƒ…å ±ã®å–å¾—ã«å¤±æ•—: {e}", file=sys.stderr)
        return {}


def check_health(url: str) -> bool:
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
    try:
        req = urllib.request.Request(f"{url}/health")
        with urllib.request.urlopen(req, timeout=5) as response:
            return response.status == 200
    except Exception:
        return False


def main():
    parser = argparse.ArgumentParser(description="ã‚¯ã‚¤ãƒƒã‚¯ã‚«ãƒŠãƒªãƒ¼ãƒ†ã‚¹ãƒˆ")
    parser.add_argument("--url", default="http://localhost:8080", help="ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³URL")
    parser.add_argument("--events", type=int, default=1000, help="é€ä¿¡ã‚¤ãƒ™ãƒ³ãƒˆæ•°")
    parser.add_argument("--keys", default="BTC,ETH,XRP", help="ã‚­ãƒ¼ (ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Š)")
    parser.add_argument("--out", required=True, help="å‡ºåŠ›JSON")

    args = parser.parse_args()

    # ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
    print(f"ğŸ” ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯: {args.url}/health")
    if not check_health(args.url):
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãŒèµ·å‹•ã—ã¦ã„ã¾ã›ã‚“", file=sys.stderr)
        sys.exit(1)
    print("âœ… ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³èµ·å‹•ç¢ºèª")
    print()

    # ã‚­ãƒ¼é…åˆ—
    keys = [k.strip() for k in args.keys.split(',')]

    # è² è·ç”Ÿæˆ
    print(f"âš¡ è² è·ãƒ†ã‚¹ãƒˆé–‹å§‹: {args.events}ã‚¤ãƒ™ãƒ³ãƒˆ")
    start_time = time.time()
    sent_count = 0

    for i in range(args.events):
        key = random.choice(keys)
        payload = {
            "symbol": key,
            "price": random.uniform(1000, 100000),
            "quantity": random.uniform(0.1, 100),
            "ts": int(time.time() * 1000)
        }

        if send_event(args.url, key, payload):
            sent_count += 1

        # é€²æ—è¡¨ç¤º
        if (i + 1) % 100 == 0:
            print(f"   é€²æ—: {i + 1}/{args.events}")

    elapsed = time.time() - start_time
    print(f"âœ… è² è·ãƒ†ã‚¹ãƒˆå®Œäº†: {sent_count}ã‚¤ãƒ™ãƒ³ãƒˆé€ä¿¡ (çµŒé: {elapsed:.1f}ç§’)")
    print()

    # ã‚¯ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³
    print("â³ ã‚¯ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³ (1ç§’)...")
    time.sleep(1)

    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹å–å¾—
    print("ğŸ“Š ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†ä¸­...")
    stats = get_stats(args.url)

    if not stats:
        print("âŒ ã‚¨ãƒ©ãƒ¼: ãƒ¡ãƒˆãƒªã‚¯ã‚¹å–å¾—å¤±æ•—", file=sys.stderr)
        sys.exit(1)

    # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœJSONç”Ÿæˆ
    fast_path_count = stats.get('fast_path_count', 0)
    drop_count = stats.get('fast_path_drop_count', 0)

    process_p50 = stats.get('fast_path_process_p50_us', 0.0)
    process_p99 = stats.get('fast_path_process_p99_us', 0.0)
    process_p999 = stats.get('fast_path_process_p999_us', 0.0)

    publish_p50 = stats.get('fast_path_publish_p50_us', 0.0)
    publish_p99 = stats.get('fast_path_publish_p99_us', 0.0)
    publish_p999 = stats.get('fast_path_publish_p999_us', 0.0)

    pq_write_p99 = stats.get('persistence_queue_write_p99_us', 0.0)
    pq_error_count = stats.get('persistence_queue_error_count', 0)
    pq_lag = stats.get('persistence_queue_lag', 0)

    # tail_ratioè¨ˆç®—
    tail_ratio = (process_p99 / process_p50) if process_p50 > 0 else 0.0

    # ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆè¨ˆç®—
    throughput = (fast_path_count / elapsed) if elapsed > 0 else 0.0

    # ã‚¨ãƒ©ãƒ¼ç‡è¨ˆç®—
    error_rate = (pq_error_count * 100.0 / fast_path_count) if fast_path_count > 0 else 0.0

    # Gitæƒ…å ±
    try:
        import subprocess
        git_commit = subprocess.check_output(['git', 'rev-parse', 'HEAD'], text=True).strip()
        git_branch = subprocess.check_output(['git', 'rev-parse', '--abbrev-ref', 'HEAD'], text=True).strip()
    except Exception:
        git_commit = "unknown"
        git_branch = "unknown"

    # çµæœJSON
    result = {
        "version": "v1",
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "environment": {
            "type": "local",
            "config": {
                "fast_path_enable": True,
                "fast_path_metrics": True,
                "kafka_bridge_enable": False,
                "jvm_heap_mb": 2048
            },
            "git_commit": git_commit,
            "git_branch": git_branch
        },
        "profile": {
            "name": "custom",
            "duration_sec": max(1, int(elapsed)),
            "events_total": args.events,
            "warmup_events": 0,
            "keys": keys
        },
        "metrics": {
            "fast_path": {
                "count": fast_path_count,
                "process_latency_us": {
                    "p50": process_p50,
                    "p99": process_p99,
                    "p999": process_p999
                },
                "publish_latency_us": {
                    "p50": publish_p50,
                    "p99": publish_p99,
                    "p999": publish_p999
                },
                "drop_count": drop_count
            },
            "persistence_queue": {
                "write_latency_us": {
                    "p99": pq_write_p99
                },
                "error_count": pq_error_count,
                "lag": pq_lag
            },
            "summary": {
                "tail_ratio": tail_ratio,
                "throughput_events_per_sec": throughput,
                "error_rate_percent": error_rate
            }
        },
        "slo_compliance": {
            "status": "PASS",
            "checks": [],
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
    }

    # å‡ºåŠ›
    output_path = Path(args.out)
    output_path.parent.mkdir(parents=True, exist_ok=True)

    with open(output_path, 'w') as f:
        json.dump(result, f, indent=2)

    print(f"ğŸ’¾ çµæœä¿å­˜: {output_path}")
    print()
    print("ğŸ“ˆ çµæœã‚µãƒãƒª:")
    print(f"   Fast Pathå‡¦ç†æ•°: {fast_path_count}")
    print(f"   ãƒ‰ãƒ­ãƒƒãƒ—æ•°: {drop_count}")
    print(f"   p50: {process_p50:.3f}Î¼s")
    print(f"   p99: {process_p99:.3f}Î¼s")
    print(f"   p999: {process_p999:.3f}Î¼s")
    print(f"   Tail Ratio: {tail_ratio:.2f}")
    print(f"   ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {throughput:.0f} events/s")
    print(f"   ã‚¨ãƒ©ãƒ¼ç‡: {error_rate:.4f}%")
    print()
    print("âœ… ã‚¯ã‚¤ãƒƒã‚¯ã‚«ãƒŠãƒªãƒ¼å®Œäº†")


if __name__ == "__main__":
    main()
